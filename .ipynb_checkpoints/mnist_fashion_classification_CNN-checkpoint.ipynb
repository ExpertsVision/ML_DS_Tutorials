{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are doing multi-class classification of mnist-fashion data using simple Convolutional Neural Networks.\n",
    "We used Keras Sequential API to build our model. There are a total of 70K samples and number of classes is 10. Each image is of size 28*28.  Most of the code is taken from Coursera deep learning course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\KHC\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion MNIST data is available in the tf.keras datasets API. load_data function call will give us data in the form of training and testing images along with their labels. Downloading of this dataset took around 10 seconds on my laptop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can see that there are  a total of 60,000 images for training and 10,000 for testing. Each image size is 28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for CNN, we need to reshape our training and testing data. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-127f55c218e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_images\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'training_images' is not defined"
     ]
    }
   ],
   "source": [
    "training_images=training_images.reshape(60000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can view different images as along with their values and labels as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label or class is: 9\n",
      "Pixel values are: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
      "    0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
      "   54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
      "  144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
      "  107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
      "  216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
      "  223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
      "  235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
      "  180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
      "  169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
      "  198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
      "  232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
      "  222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
      "  211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
      "  224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
      "  255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
      "  188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
      "  168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
      "  239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
      "  199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
      "  195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
      "  210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
      "  182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c25e1e8d68>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Image at index 0\n",
    "print('Label or class is:',training_labels[0])\n",
    "print('Pixel values are:',training_images[0])\n",
    "plt.imshow(training_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label or class is: 0\n",
      "Pixel values are: [[  0   0   0   0   0   1   0   0   0   0  41 188 103  54  48  43  87 168 133  16   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0  49 136 219 216 228 236 255 255 255 255 217 215 254 231 160  45   0   0   0   0   0]\n",
      " [  0   0   0   0   0  14 176 222 224 212 203 198 196 200 215 204 202 201 201 201 209 218 224 164   0   0   0   0]\n",
      " [  0   0   0   0   0 188 219 200 198 202 198 199 199 201 196 198 198 200 200 200 200 201 200 225  41   0   0   0]\n",
      " [  0   0   0   0  51 219 199 203 203 212 238 248 250 245 249 246 247 252 248 235 207 203 203 222 140   0   0   0]\n",
      " [  0   0   0   0 116 226 206 204 207 204 101  75  47  73  48  50  45  51  63 113 222 202 206 220 224   0   0   0]\n",
      " [  0   0   0   0 200 222 209 203 215 200   0  70  98   0 103  59  68  71  49   0 219 206 214 210 250  38   0   0]\n",
      " [  0   0   0   0 247 218 212 210 215 214   0 254 243 139 255 174 251 255 205   0 215 217 214 208 220  95   0   0]\n",
      " [  0   0   0  45 226 214 214 215 224 205   0  42  35  60  16  17  12  13  70   0 189 216 212 206 212 156   0   0]\n",
      " [  0   0   0 164 235 214 211 220 216 201  52  71  89  94  83  78  70  76  92  87 206 207 222 213 219 208   0   0]\n",
      " [  0   0   0 106 187 223 237 248 211 198 252 250 248 245 248 252 253 250 252 239 201 212 225 215 193 113   0   0]\n",
      " [  0   0   0   0   0  17  54 159 222 193 208 192 197 200 200 200 200 201 203 195 210 165   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  47 225 192 214 203 206 204 204 205 206 204 212 197 218 107   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   6   0  46 212 195 212 202 206 205 204 205 206 204 212 200 218  91   0   3   1   0   0   0]\n",
      " [  0   0   0   0   0   1   0  11 197 199 205 202 205 206 204 205 207 204 205 205 218  77   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   3   0   2 191 198 201 205 206 205 205 206 209 206 199 209 219  74   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0 188 197 200 207 207 204 207 207 210 208 198 207 221  72   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0 215 198 203 206 208 205 207 207 210 208 200 202 222  75   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 212 198 209 206 209 206 208 207 211 206 205 198 221  80   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 204 201 205 208 207 205 211 205 210 210 209 195 221  96   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 202 201 205 209 207 205 213 206 210 209 210 194 217 105   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 204 204 205 208 207 205 215 207 210 208 211 193 213 115   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 204 207 207 208 206 206 215 210 210 207 212 195 210 118   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 198 208 208 208 204 207 212 212 210 207 211 196 207 121   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 198 210 207 208 206 209 213 212 211 207 210 197 207 124   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 172 210 203 201 199 204 207 205 204 201 205 197 206 127   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 188 221 214 234 236 238 244 244 244 240 243 214 224 162   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 139 146 130 135 135 137 125 124 125 121 119 114 130  76   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1922bda6668>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Image at index 1\n",
    "print('Label or class is:',training_labels[1])\n",
    "print('Pixel values are:',training_images[1])\n",
    "plt.imshow(training_images[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is an important step if we are to use Neural networks for classification. Since maximum pixel value is 255, so we will divide all values with 255 to get a value between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building using Keras Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might comes a situation where we reach our required loss at some epoc and we don't want to proceed further. Callbacks come handy in such situations. One use of callbacks along with many others is that we can put a threshold on accuracy or loss and model will automatically stop training once it reaches the threshold value. In class below, we are using a thresold value of 0.4 for loss. Model will stop training after minimizing loss to 0.4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('loss')<0.4):\n",
    "      print(\"\\nMinimized loss to threshold value so cancelling training!\")\n",
    "      self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model and adding layers\n",
    "We are building our model with three layers. \n",
    "#### Flatten Layer:\n",
    "Our images are of size 28*28 while NN needs 1 dimensional data as input. For this we use Flatten layer that will automatically convert images data to 1D.\n",
    "\n",
    "#### Dense Layer:\n",
    "This layer adds a layer of neurons. All neurons are passed through activation functions. We are using 2 activation functions. \n",
    "\n",
    "\n",
    "512 is the number of hidden nodes in first layer. You can change this number to any other number and observe the results. If this number is too small, model will be underfitted and if this number is too large, model will be overfitted. A model with larger number of nodes will take more time in processing. \n",
    "Answer of choosing appropriate number of hidden nodes and layers can be found in this link:https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "\n",
    "#### ReLU:\n",
    "If X>0, ReLU will return X, else will return 0. \n",
    "\n",
    "#### Softmax\n",
    "It takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0]. The goal is to save a lot of coding!\n",
    "\n",
    "#### Output Layer:\n",
    "tf.keras.layers.Dense(10, activation=tf.nn.softmax)]) is the output layer. Here 10 is the number of output or total number of classes. Here we have 10 classes or target values, so we used 10 in last layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
    "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer =optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  401920    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 13s 212us/sample - loss: 0.4733 - acc: 0.8316\n",
      "Epoch 2/5\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8683\n",
      "Minimized loss to threshold value so cancelling training!\n",
      "60000/60000 [==============================] - 13s 218us/sample - loss: 0.3612 - acc: 0.8683\n"
     ]
    }
   ],
   "source": [
    "callbacks = myCallback()\n",
    "hist=model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we got accuracy of 0.8817 or around 88% on training data. We can also check accuracy and loss on test data as well. If accuracy on training data is much higher than of testing data, model is overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluated our model on testing data. We got an accuracy of around 87% on testing data. We also give single image to our model and verified that our model is giving same class of image as of actual class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 61us/sample - loss: 0.3556 - acc: 0.8695\n",
      "Predicted class of test image at index 0 is: 9\n",
      "Actual class of test image at index 0 is: 9\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)\n",
    "classifications = model.predict(test_images)\n",
    "pred_probs=classifications[0]\n",
    "pred_class=np.where(classifications[0] == np.amax(classifications[0]))\n",
    "\n",
    "print('Predicted class of test image at index 0 is:',pred_class[0][0])\n",
    "print('Actual class of test image at index 0 is:',test_labels[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
